# Mock Exam 1

You don't get what you wish for. You get what you work for.

– Daniel Milstein

I am experienced enough to do this. I am knowledgeable enough to do this. I am prepared enough to do this. I am mature enough to do this. I am brave enough to do this.

1. Create a Pod **mc-pod** in the **mc-namespace** namespace with three containers. The first container should be named **mc-pod-1**, run the **nginx:1-alpine** image, and set an environment variable **NODE_NAME** to the node name. The second container should be named **mc-pod-2**, run the **busybox:1** image, and continuously log the output of the date command to the file ``` /var/log/shared/date.log ``` every second. The third container should have the name **mc-pod-3**, run the image **busybox:1**, and print the contents of the **date.log** file generated by the second container to stdout. Use a shared, non-persistent volume.

**Solution**

You need to create a Pod named mc-pod in the mc-namespace namespace with three containers:

1.1 First container (mc-pod-1):

Uses the image **nginx:1-alpine**.
Sets an environment variable **NODE_NAME** to the node name where the pod is running.
This is done using the fieldRef:

```yaml
env:
- name: NODE_NAME
  valueFrom:
    fieldRef:
      fieldPath: spec.nodeName
```

1.2 Second container (mc-pod-2):

Uses the image **busybox:1**.
Runs a command that writes the output of the date command to ``` /var/log/shared/date.log ``` every second in an infinite loop.
Example command:

```yaml
- name: mc-pod-2
  image: busybox:1
  command: ["sh", "-c", "while true; do date >> /var/log/shared/date.log; sleep 1; done"]
  volumeMounts:
  - name: shared-logs
    mountPath: /var/log/shared
```

Mounts a shared volume at /var/log/shared.

1.3 Third container (mc-pod-3):

Uses the image **busybox:1**.
Reads and prints the contents of ```/var/log/shared/date.log ``` to stdout.
Example command:

```bash
command: ["sh", "-c", "tail -f /var/log/shared/date.log"]
```

Also mounts the same shared volume at ``` /var/log/shared ```.

All three containers share a non-persistent (emptyDir) volume mounted at ``` /var/log/shared ```.


Yaml manifest:

```yaml
# 01_mc-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: mc-pod
  namespace: mc-namespace
spec:
  containers:
  - name: mc-pod-1
    image: nginx:1-alpine
    env:
    - name: NODE_NAME
      valueFrom:
        fieldRef:
          fieldPath: spec.nodeName
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/shared
  - name: mc-pod-2
    image: busybox:1
    command: ["sh", "-c", "while true; do date >> /var/log/shared/date.log; sleep 1; done"]
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/shared
  - name: mc-pod-3
    image: busybox:1
    command: ["sh", "-c", "tail -f /var/log/shared/date.log"]
    volumeMounts:
    - name: shared-logs
      mountPath: /var/log/shared 
  volumes:
  - name: shared-logs
    emptyDir: {}
```

```bash
# -l line numbers activated.
# Alternatively, while the file is open, you can press ALT + C to display the current line number at the bottom of the editor. 
nano -l mc-pod.yaml

k apply --validate=true -f mc-pod.yaml
```

Command to access a specific container
To run an interactive shell in one of the containers, use:

```sh
kubectl exec -it mc-pod -n mc-namespace -c mc-pod-1 -- sh
```

You can change mc-pod-1 to mc-pod-2 or mc-pod-3 depending on the container you want to inspect.

View the contents of /var/log/shared
Once inside the container, you can list the files:

```sh
ls -l /var/log/shared
```

And if you want to see the contents of the file generated by mc-pod-2:

```sh
cat /var/log/shared/date.log
```
Or follow it in real time (although mc-pod-3 already does that):

```sh
tail -f /var/log/shared/date.log
```

[Ref multi container yaml example](https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/#creating-a-pod-that-runs-two-containers)

2. This question needs to be solved on node node01. To access the node using SSH, use the credentials below:

```bash
username: bob
password: caleston123
```

As an administrator, you need to prepare **node01** to install kubernetes. One of the steps is installing a container runtime. Install the **cri-docker_0.3.16.3-0.debian.deb** package located in /root and ensure that the **cri-docker** service is running and enabled to start on boot.

**Solution**

2.1. SSH into node01
Use the provided credentials:

```sh
username: bob
password: caleston123
```
2.2 Switch to root user (if needed):

```sh
sudo -i
```

2.3 Install the cri-docker package:

```sh
dpkg -i /root/cri-docker_0.3.16.3-0.debian.deb
```

2.4 Start the cri-docker service:

```sh
systemctl start cri-docker
```

2.5 Enable the cri-docker service to start on boot:

```sh
systemctl enable cri-docker
```

2.6 Verify the service is running:

```sh
systemctl status cri-docker
```

3. On controlplane node, identify all CRDs related to VerticalPodAutoscaler and save their names into the file /``` root/vpa-crds.txt ```.

**Solution**

3.1 List all CRDs and filter those related to VerticalPodAutoscaler:

Use the following command to list all CRDs and filter for those containing "verticalpodautoscaler" (case-insensitive):

```sh
kubectl get crds | grep -i verticalpodautoscaler
```

3.2 Extract only the CRD names and save them to the file:

You can use awk to get just the first column (the CRD name):

```sh
kubectl get crds | grep -i verticalpodautoscaler | awk '{print $1}' > /root/vpa-crds.txt
```

**grep -i verticalpodautoscaler**

Filters the above output to show only lines containing the text verticalpodautoscaler, regardless of case (-i is for ignore case).

**awk '{print $1}'**

Of each filtered line, awk prints only the first column (field), which is usually the CRD name.


3.3 Verify the contents of the file:

```sh
cat /root/vpa-crds.txt
```

4. Create a service **messaging-service** to expose the messaging application within the cluster on port **6379**.

**Solution**

4.1. First, identify the messaging application (pod/deployment):

```sh
kubectl get pods -l app=messaging
# or
kubectl get pods | grep messaging
```

4.2. Check the labels of the messaging pod/deployment:

```sh
kubectl get pods --show-labels | grep messaging
```

4.3. Create the service using imperative command:

```sh
kubectl expose pod <messaging-pod-name> --name=messaging-service --port=6379 --target-port=6379
```

OR if it's a deployment:

```sh
kubectl expose deployment <messaging-deployment-name> --name=messaging-service --port=6379 --target-port=6379
```

4.4. Alternative: Create service using YAML manifest:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: messaging-service
spec:
  selector:
    app: messaging  # Use the correct label from step 2
  ports:
  - protocol: TCP
    port: 6379
    targetPort: 6379
```

4.5. Verify the service was created:

```sh
kubectl get service messaging-service
kubectl describe service messaging-service
```

**Summary:**
You create a ClusterIP service named **messaging-service** that exposes the messaging application on port **6379**. The service uses selectors to target the messaging pods and routes traffic to port **6379** on those pods. This makes the messaging application accessible within the cluster via the service name.

5. Create a deployment named hr-web-app using the image **kodekloud/webapp-color** with 2 replicas.

**Solution**

5.1. Method 1: Using imperative command (recommended for speed):

```sh
kubectl create deployment hr-web-app --image=kodekloud/webapp-color --replicas=2
```

5.2. Method 2: Using YAML manifest:

```yaml
# hr-web-app.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hr-web-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: hr-web-app
  template:
    metadata:
      labels:
        app: hr-web-app
    spec:
      containers:
      - name: hr-web-app
        image: kodekloud/webapp-color
```

5.3. Then apply it:

```sh
kubectl apply -f hr-web-app.yaml
```

**Verification:**

Check that the deployment was created successfully:

```sh
kubectl get deployment hr-web-app
kubectl get pods -l app=hr-web-app
```


6. A new application orange is deployed. There is something wrong with it. Identify and fix the issue.

**Solution**

6.1. First, identify the orange application and check its status:

```sh
kubectl get pods | grep orange
kubectl get deployments | grep orange
kubectl get services | grep orange
```

6.2. Check the pod status and events:

```sh
kubectl describe pod <orange-pod-name>
# Look for error messages in the "Events" section.
```

6.3. Check the logs of the orange pod:

```sh
kubectl logs <orange-pod-name>
```

6.4. Common issues to look for:

- Image pull errors: Wrong image name or tag
- Configuration errors: Missing environment variables, wrong ports
- Resource issues: Insufficient CPU/memory limits
- Volume mount issues: Missing volumes or incorrect paths
- Service connectivity: Wrong selectors or ports
- If it's a deployment issue, check the deployment:

6.5. If it's a deployment issue, check the deployment:

```sh
kubectl describe deployment <orange-deployment-name>
```

6.6. Fix the issue based on what you find:

- Edit the deployment: kubectl edit deployment <orange-deployment-name>
- Or edit the pod: kubectl edit pod <orange-pod-name>
- Or recreate the resource with correct configuration

6.7. Verify the fix:

```sh
kubectl get pods | grep orange
kubectl logs <orange-pod-name>
```

**Summary:**
You troubleshoot by checking pod status, describing resources, and examining logs to identify the root cause. Common issues include image problems, configuration errors, or resource constraints. Once identified, you fix the issue by editing the resource or recreating it with the correct configuration.


7. Expose the **hr-web-app** created in the previous task as a service named **hr-web-app-service**, accessible on port **30082** on the nodes of the cluster.

The web application listens on port **8080**.

**Solution**

7.1. Method 1: Using imperative command (recommended for speed):

```sh
kubectl expose deployment hr-web-app --name=hr-web-app-service --type=NodePort --port=8080 --target-port=8080
```

Then edit the service to set the specific NodePort:

```sh
kubectl patch service hr-web-app-service --type='json' -p='[{"op": "replace", "path": "/spec/ports/0/nodePort", "value": 30082}]'
```

7.2. Method 2: Using YAML manifest:

```yaml
# hr-web-app-service
apiVersion: v1
kind: Service
metadata:
  name: hr-web-app-service
spec:
  type: NodePort
  selector:
    app: hr-web-app
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 8080
    nodePort: 30082
```

Then apply it:

```sh
kubectl apply -f hr-web-app-service.yaml
```

Verification:

Check that the service was created successfully:

```sh
kubectl get service hr-web-app-service
kubectl describe service hr-web-app-service
```

You should see the service with NodePort: **30082** and TargetPort: **8080**.


8. Create a Persistent Volume with the given specification: -

- Volume name: pv-analytics
- Storage: 100Mi
- Access mode: ReadWriteMany
- Host path: /pv/data-analytics

**Solution**

8.1. Method: Create YAML manifest for the PersistentVolume:

Create a YAML file (e.g., pv-analytics.yaml):

```yaml
# pv-analytics.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-analytics
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteMany
  hostPath:
    path: /pv/data-analytics
  persistentVolumeReclaimPolicy: Retain
```

You can follow an example: https://kubernetes.io/docs/concepts/storage/persistent-volumes/

8.2. Apply the manifest:

```sh
kubectl apply -f pv-analytics.yaml
```

Verification: Check that the PersistentVolume was created successfully:

```sh
kubectl get pv pv-analytics
kubectl describe pv pv-analytics
```

You should see the PV with:

- Capacity: 100Mi
- Access Modes: RWX (ReadWriteMany)
- Status: Available
- Path: /pv/data-analytics

**Summary:**
You create a PersistentVolume named **pv-analytics** with **100Mi** storage capacity, **ReadWriteMany** access mode, and a **hostPath** pointing to **/pv/data-analytics**. The hostPath type means the volume uses a directory on the host node's filesystem. The ReadWriteMany access mode allows multiple pods to mount this volume simultaneously for read and write operations.

9. Create a Horizontal Pod Autoscaler (HPA) with name **webapp-hpa** for the deployment named **kkapp-deploy** in the default namespace with the **webapp-hpa.yaml** file located under the root folder.
Ensure that the HPA scales the deployment based on CPU utilization, maintaining an average CPU usage of 50% across all pods.
Configure the HPA to cautiously scale down pods by setting a stabilization window of **300** seconds to prevent rapid fluctuations in pod count.

Note: The **kkapp-deploy** deployment is created for backend; you can check in the terminal.

**Solution**

9.1. Method 1: Using imperative command (basic HPA):

```sh
kubectl autoscale deployment kkapp-deploy --name=webapp-hpa --cpu-percent=50 --min=1 --max=10
```

Then edit the HPA to add the stabilization window:

```sh
kubectl edit hpa webapp-hpa
```

9.2. Method 2: Create YAML manifest (recommended for complete configuration):

```yaml
# webapp-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: webapp-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: kkapp-deploy
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
```

Apply the manifest:

```sh
kubectl apply -f /root/webapp-hpa.yaml
```

Verification:

```sh
kubectl get hpa webapp-hpa
kubectl describe hpa webapp-hpa
```

**Summary:**
You create an HPA that monitors the **kkapp-deploy** deployment and maintains **50%** average CPU utilization across all pods. The stabilizationWindowSeconds: **300** prevents rapid scale-down operations by waiting **5** minutes before scaling down, ensuring stability and preventing pod thrashing.

[Ref horizontal-pod-autoscale-walkthrough](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/)



10. Deploy a Vertical Pod Autoscaler (VPA) with name **analytics-vpa** for the deployment named **analytics-deployment** in the default namespace.
The VPA should automatically adjust the CPU and memory requests of the pods to optimize resource utilization. Ensure that the VPA operates in Auto mode, allowing it to evict and recreate pods with updated resource requests as needed.

**Solution**


10.1. Method: Create YAML manifest for the VerticalPodAutoscaler:

Create a YAML file (e.g., analytics-vpa.yaml)

```yaml
# analytics-vpa.yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: analytics-vpa
  namespace: default
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: analytics-deployment
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: '*'
      maxAllowed:
        cpu: 1
        memory: 500Mi
      minAllowed:
        cpu: 100m
        memory: 50Mi
      controlledResources: ["cpu", "memory"]

```

Apply the manifest:

```sh
kubectl apply -f analytics-vpa.yaml
```

Verification: Check that the VPA was created successfully:

```sh
kubectl get vpa analytics-vpa
kubectl describe vpa analytics-vpa
```

You should see the VPA with:

- Target: analytics-deployment
- Update Mode: Auto
- Status: Should show recommendations for CPU and memory

**Summary:**
You create a VerticalPodAutoscaler named analytics-vpa that monitors the analytics-deployment and operates in Auto mode. This means the VPA will automatically evict pods and recreate them with updated CPU and memory requests based on actual usage patterns. The VPA optimizes resource utilization by right-sizing the resource requests for the containers in the deployment.


11. Create a Kubernetes Gateway resource with the following specifications:

- Name: **web-gateway**
- Namespace: **nginx-gateway**
- Gateway Class Name: **nginx**
- Listeners:
    - Protocol: **HTTP**
    - Port: **80**
    - Name: **http**
  
**Solution**

11.1. Method: Create YAML manifest for the Gateway resource:

Note: Make sure the namespace exists first:

```sh
kubectl create namespace nginx-gateway
```

Create a YAML file (e.g., web-gateway.yaml):

```yaml
# web-gateway.yaml
apiVersion: gateway.networking.k8s.io/v1beta1
kind: Gateway
metadata:
  name: web-gateway
  namespace: nginx-gateway
spec:
  gatewayClassName: nginx
  listeners:
  - name: http
    port: 80
    protocol: HTTP
```

Apply the manifest:

```sh
kubectl apply -f web-gateway.yaml
```

Verification: Check that the Gateway was created successfully:

```sh
kubectl get gateway web-gateway -n nginx-gateway
kubectl describe gateway web-gateway -n nginx-gateway
```

12. One co-worker deployed an nginx helm chart **kk-mock1** in the **kk-ns** namespace on the cluster. A new update is pushed to the helm chart, and the team wants you to update the helm repository to fetch the new changes.

After updating the helm chart, upgrade the helm chart version to 18.1.15.

**Solution**

Step 1: List current helm releases to identify the chart:

```sh
helm list -n kk-ns
```

Step 2: Check which repository the chart came from:

```sh
helm get values kk-mock1 -n kk-ns
# or
helm status kk-mock1 -n kk-ns
```

Step 3: Update the helm repository to fetch new changes:

```sh
helm repo update
# This command updates all configured repositories to fetch the latest chart versions.
```

Step 4: Check available versions of the nginx chart:

```sh
helm search repo nginx --versions
```

Step 5: Upgrade the helm chart to version 18.1.15:

```sh
helm upgrade kk-mock1 nginx --version 18.1.15 -n kk-ns
```

Step 6: Verify the upgrade was successful:

```sh
helm list -n kk-ns
helm status kk-mock1 -n kk-ns
```

Alternative method if you need to specify the repository:

```sh
helm upgrade kk-mock1 <repository-name>/nginx --version 18.1.15 -n kk-ns
```

**Summary:**
You update the helm repository using helm repo update to fetch the latest chart information, then upgrade the existing **kk-mock1** release to version **18.1.15** using helm upgrade. This ensures the **nginx** chart is updated to the specified version while maintaining the existing configuration and deployment in the **kk-ns** namespace.












